{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9ba64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "import zipfile\n",
    "import pathlib\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4d6cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd4534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"/kaggle/input/ai-of-god-3/Public_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7465130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = data_path/\"train_images\"\n",
    "test_path = data_path/\"test_images\"\n",
    "train_csv = data_path/\"train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d167f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7797da49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, pathlib\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root, csv_file, tokenizer=None, transform=None, mode=\"train\", max_length=32):\n",
    "        self.root = root\n",
    "        self.paths = list(pathlib.Path(self.root).glob(\"*.png\"))\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mode = mode\n",
    "        self.max_length = max_length\n",
    "        self.labels = {'id': [], 'transcription': []}\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            with open(csv_file, mode='r') as file:\n",
    "                reader = csv.DictReader(file)\n",
    "                for row in reader:\n",
    "                    self.labels['id'].append(row['unique Id'])\n",
    "                    self.labels['transcription'].append(row['transcription'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.paths[index]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.mode == \"train\":\n",
    "            image_id = os.path.splitext(os.path.basename(image_path))[0]\n",
    "            transcription = self.labels['transcription'][self.labels['id'].index(image_id)]\n",
    "            encoding = self.tokenizer(transcription, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "            input_ids = encoding[\"input_ids\"].squeeze(0)  # [max_len]\n",
    "            return image, input_ids\n",
    "\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b0fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # or custom tokenizer\n",
    "pad_token_id = vocab_tokenizer.pad_token_id\n",
    "bos_token_id = vocab_tokenizer.bos_token_id\n",
    "\n",
    "# Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9b30b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CustomImageDataset(root=\"train/images\", csv_file=\"train/labels.csv\", tokenizer=vocab_tokenizer, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02145ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "train_size = int(0.8 * len(data))\n",
    "test_size = len(data) - train_size\n",
    "\n",
    "train_data, test_data = random_split(data, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e7ab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "BATCH_SIZE = 32\n",
    "train_dataloader = DataLoader(dataset=train_data, # use custom created train Dataset\n",
    "                                     batch_size=BATCH_SIZE, # how many samples per batch?\n",
    "                                     num_workers=NUM_WORKERS, # how many subprocesses to use for data loading? (higher = more)\n",
    "                                     shuffle=True) # shuffle the data?\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data, # use custom created test Dataset\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    num_workers=NUM_WORKERS,\n",
    "                                    shuffle=False) # don't usually need to shuffle testing data\n",
    "\n",
    "train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a3596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import math\n",
    "\n",
    "# --- Positional Encoding ---\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)  # [max_len, d_model]\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # [max_len, 1]\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))  # [d_model/2]\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)].to(x.device)\n",
    "        return x\n",
    "\n",
    "# --- CNN Encoder using ResNet ---\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, output_dim=512):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-2])  # Remove avgpool and fc\n",
    "        self.project = nn.Conv2d(512, output_dim, kernel_size=1)  # project to model_dim\n",
    "\n",
    "    def forward(self, x):  # x: [B, 3, H, W]\n",
    "        features = self.features(x)  # [B, 512, H/32, W/32]\n",
    "        features = self.project(features)  # [B, output_dim, H', W']\n",
    "        B, C, H, W = features.size()\n",
    "        features = features.permute(0, 2, 3, 1).reshape(B, H*W, C)  # [B, HW, C]\n",
    "        return features  # sequence of visual tokens\n",
    "\n",
    "# --- Transformer Decoder ---\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, model_dim=512, nhead=8, num_layers=6, max_len=256):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, model_dim)\n",
    "        self.pos_encoder = PositionalEncoding(model_dim, max_len)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=model_dim, nhead=nhead)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "        self.fc_out = nn.Linear(model_dim, vocab_size)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        tgt_emb = self.embedding(tgt)  # [T, B, D]\n",
    "        tgt_emb = self.pos_encoder(tgt_emb)\n",
    "        output = self.transformer_decoder(tgt_emb, memory, tgt_mask=tgt_mask, memory_mask=memory_mask)\n",
    "        return self.fc_out(output)\n",
    "\n",
    "# --- Full OCR Model ---\n",
    "class CNNTransformerOCR(nn.Module):\n",
    "    def __init__(self, vocab_size, model_dim=512, max_len=256):\n",
    "        super().__init__()\n",
    "        self.encoder = CNNEncoder(output_dim=model_dim)\n",
    "        self.decoder = TransformerDecoder(vocab_size, model_dim=model_dim, max_len=max_len)\n",
    "        self.model_dim = model_dim\n",
    "\n",
    "    def forward(self, images, tgt_seq, tgt_mask=None):\n",
    "        memory = self.encoder(images)  # [B, S, D]\n",
    "        memory = memory.permute(1, 0, 2)  # [S, B, D] for Transformer\n",
    "        tgt_seq = tgt_seq.permute(1, 0)  # [T, B]\n",
    "        output = self.decoder(tgt_seq, memory, tgt_mask)\n",
    "        return output  # [T, B, vocab_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ad100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNTransformerOCR(vocab_size=len(vocab_tokenizer)).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ed1694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wer(reference, hypothesis):\n",
    "    # Split the reference and hypothesis sentences into words\n",
    "    reference_words = reference.split()\n",
    "    hypothesis_words = hypothesis.split()\n",
    "\n",
    "    # Create a matrix to store the distances\n",
    "    d = np.zeros((len(reference_words) + 1, len(hypothesis_words) + 1), dtype=np.uint8)\n",
    "\n",
    "    # Initialize the matrix\n",
    "    for i in range(1, len(reference_words) + 1):\n",
    "        d[i][0] = i\n",
    "    for j in range(1, len(hypothesis_words) + 1):\n",
    "        d[0][j] = j\n",
    "\n",
    "    # Fill the matrix\n",
    "    for i in range(1, len(reference_words) + 1):\n",
    "        for j in range(1, len(hypothesis_words) + 1):\n",
    "            if reference_words[i - 1] == hypothesis_words[j - 1]:\n",
    "                d[i][j] = d[i - 1][j - 1]\n",
    "            else:\n",
    "                d[i][j] = min(d[i - 1][j], d[i][j - 1], d[i - 1][j - 1]) + 1\n",
    "\n",
    "    wer_value = d[len(reference_words)][len(hypothesis_words)] / len(reference_words)\n",
    "\n",
    "    return wer_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91787332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_dataloader, val_dataloader, tokenizer, optimizer, criterion, device, num_epochs=10, pad_token_id=0):\n",
    "    best_wer = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for images, tgt_input_ids in tqdm(train_dataloader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
    "            images = images.to(device)\n",
    "            tgt_input_ids = tgt_input_ids.to(device)\n",
    "\n",
    "            tgt_input = tgt_input_ids[:, :-1]  # [B, T-1]\n",
    "            tgt_output = tgt_input_ids[:, 1:]  # [B, T-1]\n",
    "\n",
    "            tgt_input = tgt_input.permute(1, 0)  # [T, B]\n",
    "            tgt_output = tgt_output.permute(1, 0)  # [T, B]\n",
    "\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_input.size(0)).to(device)\n",
    "\n",
    "            logits = model(images, tgt_input, tgt_mask=tgt_mask)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), tgt_output.reshape(-1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "        print(f\"[Epoch {epoch+1}] Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        references = []\n",
    "        predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, tgt_input_ids in tqdm(val_dataloader, desc=f\"Epoch {epoch+1} [Val]\"):\n",
    "                images = images.to(device)\n",
    "                tgt_input_ids = tgt_input_ids.to(device)\n",
    "                batch_size = images.size(0)\n",
    "\n",
    "                # Decode token-by-token\n",
    "                decoded = torch.full((batch_size, 1), tokenizer.bos_token_id, dtype=torch.long).to(device)\n",
    "                memory = model.encoder(images).permute(1, 0, 2)  # [S, B, D]\n",
    "\n",
    "                for _ in range(tgt_input_ids.size(1)):\n",
    "                    tgt_mask = nn.Transformer.generate_square_subsequent_mask(decoded.size(1)).to(device)\n",
    "                    output = model.decoder(decoded.permute(1, 0), memory, tgt_mask=tgt_mask)\n",
    "                    next_token = output.argmax(-1)[-1, :]  # [B]\n",
    "                    decoded = torch.cat([decoded, next_token.unsqueeze(1)], dim=1)\n",
    "\n",
    "                for i in range(batch_size):\n",
    "                    pred_text = tokenizer.decode(decoded[i].tolist(), skip_special_tokens=True)\n",
    "                    true_text = tokenizer.decode(tgt_input_ids[i].tolist(), skip_special_tokens=True)\n",
    "                    predictions.append(pred_text)\n",
    "                    references.append(true_text)\n",
    "\n",
    "                # Optional loss (for logging)\n",
    "                tgt_input = tgt_input_ids[:, :-1]\n",
    "                tgt_output = tgt_input_ids[:, 1:]\n",
    "                tgt_input = tgt_input.permute(1, 0)\n",
    "                tgt_output = tgt_output.permute(1, 0)\n",
    "                tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_input.size(0)).to(device)\n",
    "                logits = model(images, tgt_input, tgt_mask=tgt_mask)\n",
    "                loss = criterion(logits.view(-1, logits.size(-1)), tgt_output.reshape(-1))\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        print(f\"[Epoch {epoch+1}] Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Calculate Average WER\n",
    "        wers = [wer(ref, hyp) for ref, hyp in zip(references, predictions)]\n",
    "        avg_wer = sum(wers) / len(wers)\n",
    "        print(f\"[Epoch {epoch+1}] Val WER: {avg_wer:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if avg_wer < best_wer:\n",
    "            best_wer = avg_wer\n",
    "            torch.save(model.state_dict(), \"best_ocr_model.pt\")\n",
    "            print(f\"âœ… Best model saved with WER: {best_wer:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699cd95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validate(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=test_dataloader,\n",
    "    tokenizer=vocab_tokenizer,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    num_epochs=10,\n",
    "    pad_token_id=vocab_tokenizer.pad_token_id\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
